{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f9667473-7f9f-40e3-a475-5bcf98e45dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import ast \n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56496d0",
   "metadata": {},
   "source": [
    "# Begin Processing Auto-Eval Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "01cd2c43-cfec-4fbf-8c6e-a9f7a703ebbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook does several things, including:\n",
    "    # 1. Removes precision and T2V annotations for instances where coverage was set to -1 due to display issues\n",
    "    # 2. Joins in the utility and fluency results from the re-done human evaluation\n",
    "    # 2. Ensures each dataset is represented by 120 query-generation pairs per OP instantiation\n",
    "    # 3. Joins in Vertex API results about which sentences require citation (resolves discrepancies in sentence parsing)\n",
    "    # 4. Creates a copy of results with data only for sentences requiring citation\n",
    "\n",
    "baselines = False\n",
    "model_type = 'sonnet4.5'  # 'gpt4', 'gpt5', 'sonnet4.5'\n",
    "\n",
    "\n",
    "data_str_ls = ['nq', 'multihop', 'mash', 'eli3g']\n",
    "\n",
    "def op_fps(data_str, model_type):  \n",
    "     mapped_data_str = {'nq':'nq', 'multihop':'mh', 'mash':'mash', 'eli3g':'eli3'}[data_str]\n",
    "     return f'../../attrib/autoEval_results/RESOLVED_with_needs_citation_labels_{mapped_data_str}_{model_type}'\n",
    "    \n",
    "def baseline_fps(data_str):\n",
    "        mapped_data_str = {'nq':'nq', 'multihop':'mh', 'mash':'mash', 'eli3g':'eli3'}[data_str]\n",
    "        return f'../../attrib/autoEval_results/RESOLVED_with_needs_citation_labels_baseline_{mapped_data_str}_gpt4'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137492f3-6f19-435a-97eb-338038468cc4",
   "metadata": {},
   "source": [
    "# Remove irrelevant T2V and precision annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "bd0cb082-3931-4798-afff-d76d16f5993f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_irrelevant_t2v_and_precision_annotations(df):\n",
    "    # Occasionally, the annotation interface fails to display a cited sentence (coverage = -1). \n",
    "    # In these cases, precision and T2V were still collected. This function identifies and removes these measurements.\n",
    "    idxs_ops_of_interest = []\n",
    "    for i in range(len(df)):\n",
    "        if (df['op'].iloc[i] == 'Snippet'):\n",
    "            continue\n",
    "        t2vs = eval(df['t2v_coverage'].iloc[i])\n",
    "        is_covered = eval(df['is_covered'].iloc[i])\n",
    "        is_precise = eval(df['precise_citations'].iloc[i])\n",
    "        actual_is_covered = []\n",
    "        actual_is_precise = []\n",
    "        for j in range(len(is_covered)):\n",
    "            cov_item = is_covered[j]\n",
    "            if (cov_item['coverage'] != -1):\n",
    "                actual_is_covered.append(cov_item)\n",
    "            prec_item = is_precise[j]\n",
    "            if (len(prec_item['annotations'])!=0):\n",
    "                actual_is_precise.append(prec_item)\n",
    "            \n",
    "        if ((len(actual_is_covered) != len(t2vs)) or \\\n",
    "            (len(actual_is_covered) != len(actual_is_precise))):\n",
    "            query_id = df['query_id'].iloc[i]\n",
    "            op = df['op'].iloc[i]\n",
    "            idxs_ops_of_interest.append((query_id, op))\n",
    "            is_precise = eval(df['precise_citations'].iloc[i])\n",
    "            new_is_precise = []\n",
    "            for j in range(len(is_covered)):\n",
    "                coverage_item = is_covered[j]\n",
    "                if (coverage_item['coverage']!=-1):\n",
    "                    new_is_precise.append(is_precise[j])\n",
    "                else:\n",
    "                    new_is_precise.append({\"annotations\":[],\"sentence_id\":coverage_item[\"sentence_id\"]})\n",
    "    \n",
    "            if (len(is_covered)==len(t2vs)): # all of the sentences have a citation, but some weren't displayed properly\n",
    "                new_t2vs = []\n",
    "                for j in range(len(is_covered)):\n",
    "                    coverage_item = is_covered[j]\n",
    "                    if (coverage_item['coverage']!=-1):\n",
    "                        new_t2vs.append(t2vs[j])\n",
    "\n",
    "            elif (len(is_covered) > len(t2vs)): # some of the sentences had no citations and some weren't displayed properly\n",
    "                new_t2vs = []\n",
    "                k = 0 # will be used to index into t2vs\n",
    "                for j in range(len(is_covered)):\n",
    "                    coverage_item = is_covered[j]\n",
    "                    precision_item = is_precise[j]\n",
    "                    if (not ((coverage_item['coverage']==-1) and (len(precision_item['annotations'])==0))): # if T2V recorded for this sentence\n",
    "                        if (coverage_item['coverage']!=-1): # if the sentence was displayed correctly\n",
    "                            new_t2vs.append(t2vs[k]) # keep the corresponding t2v\n",
    "                        k += 1\n",
    "            else:\n",
    "                print('!!!!!! not handled')    \n",
    "                \n",
    "            \n",
    "            df['precise_citations'].iloc[i] = str(new_is_precise)\n",
    "            df['t2v_coverage'].iloc[i] = str(new_t2vs)\n",
    "\n",
    "    # print('Corrected:', idxs_ops_of_interest)\n",
    "    df = df.reset_index()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "086c7d73-4bad-46cc-90c3-aa697fd0b1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_annotations(df):\n",
    "    # checks whether all precision and T2V annotations are consistent with the coverage dict\n",
    "    df = df[df['op']!='Snippet']\n",
    "    \n",
    "    idxs_ops_of_interest = []\n",
    "    for i in range(len(df)):\n",
    "        t2vs = eval(df['t2v_coverage'].iloc[i])\n",
    "        is_covered = eval(df['is_covered'].iloc[i])\n",
    "        is_precise = eval(df['precise_citations'].iloc[i])\n",
    "        actual_is_covered = []\n",
    "        actual_is_precise = []\n",
    "        for j in range(len(is_covered)):\n",
    "            cov_item = is_covered[j]\n",
    "            if (cov_item['coverage'] != -1):\n",
    "                actual_is_covered.append(cov_item)\n",
    "            prec_item = is_precise[j]\n",
    "            if (len(prec_item['annotations'])!=0):\n",
    "                actual_is_precise.append(prec_item)\n",
    "        if (len(actual_is_covered) != len(t2vs)):\n",
    "            print(df['query_id'].iloc[i])\n",
    "            print('is_precise', is_precise)\n",
    "            print('actual_is_covered len', len(actual_is_covered))\n",
    "            print('actual_is_covered', actual_is_covered)\n",
    "            print('is_covered len', len(is_covered))\n",
    "            print('t2vs', t2vs)\n",
    "            print('t2vs len', len(t2vs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "c9bc6078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checked that annotations are consistent across precision, T2V, and coverage.\n"
     ]
    }
   ],
   "source": [
    "df_dict = {}\n",
    "\n",
    "for data_str in data_str_ls:\n",
    "\n",
    "    if baselines:\n",
    "        df_dict[data_str] = pd.read_csv(baseline_fps(data_str)+'.csv', index_col=False)\n",
    "        df_dict[data_str].rename(columns={'auto_fluency_rating': 'human_fluency_rating',\n",
    "                   'auto_utility_rating': 'human_utility_rating',\n",
    "                   'auto_precise_citations': 'precise_citations',\n",
    "                   'auto_is_covered': 'is_covered',\n",
    "                   'auto_t2v_coverage': 't2v_coverage',\n",
    "                   'auto_t2v_precision': 't2v_precision'\n",
    "                   }, inplace=True)\n",
    "\n",
    "    else:\n",
    "        df_dict[data_str] = pd.read_csv(op_fps(data_str, model_type)+'.csv', index_col=False)\n",
    "        df_dict[data_str] = df_dict[data_str].rename(columns={'auto_fluency_rating': 'human_fluency_rating',\n",
    "                    'auto_utility_rating': 'human_utility_rating',\n",
    "                    'auto_precise_citations': 'precise_citations',\n",
    "                    'auto_is_covered': 'is_covered',\n",
    "                    'auto_t2v_coverage': 't2v_coverage',\n",
    "                    'auto_t2v_precision': 't2v_precision'\n",
    "                    })\n",
    "\n",
    "for data_str in data_str_ls:\n",
    "    # df_dict[data_str] = remove_irrelevant_t2v_and_precision_annotations(df_dict[data_str])\n",
    "    check_annotations(df_dict[data_str])\n",
    "        \n",
    "print('Checked that annotations are consistent across precision, T2V, and coverage.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "f77358de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_mismatches(df):\n",
    "    # The Vertex API sometimes parses sentences differently than our implementation. We resolve these conflicts by hand.\n",
    "    ids_that_need_editing = []\n",
    "    for i in range(len(df)):\n",
    "        if (df['op'].iloc[i] == 'Snippet'):\n",
    "            continue\n",
    "        # print(\"!!!!!\", eval(df['t2v_coverage'].iloc[i]))\n",
    "        coverage_annotation_count = len(eval(df['t2v_coverage'].iloc[i])) # Sent\n",
    "        gpt4_sentence_count = len(eval(df['Sent'].iloc[i])) # Sent\n",
    "        vertex_sentence_count = len(eval(df['Sentences Need Citation'].iloc[i]))\n",
    "        if (gpt4_sentence_count != vertex_sentence_count):\n",
    "            # if ((all(eval(df['Sentences Need Citation'].iloc[i]))) & (vertex_sentence_count > gpt4_sentence_count)):\n",
    "            #     pass\n",
    "            # else:\n",
    "            print('MISMATCH')\n",
    "            print(i)\n",
    "            print('(\\''+df['op'].iloc[i]+'\\', \\''+str(df['query_id'].iloc[i])+'\\')')\n",
    "            print('curr sentence count:', gpt4_sentence_count)\n",
    "            print('coverage annotation count:', coverage_annotation_count)\n",
    "            print('sentences:', df['Sent'].iloc[i])\n",
    "            print('vertex sentence count:', vertex_sentence_count)\n",
    "            print('curr vertex label:', eval(df['Sentences Need Citation'].iloc[i]))\n",
    "            print()\n",
    "            ids_that_need_editing.append(i)\n",
    "    print(ids_that_need_editing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "a03f6272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "# Display the cases that require review\n",
    "for data_str in data_str_ls:\n",
    "    df = df_dict[data_str]\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        # if the op is 'Snippet', skip\n",
    "        if (df['op'].iloc[i] == 'Snippet'):\n",
    "            continue\n",
    "        sentences_need_citation = eval(df['Sentences Need Citation'].iloc[i])\n",
    "        if not len(eval(df['is_covered'].iloc[i])) == len(sentences_need_citation):\n",
    "            print(f\"COVERAGE data: {data_str}, id: {i}, query_id: {df['query_id'].iloc[i]}, op: {df['op'].iloc[i]} | {len(eval(df['is_covered'].iloc[i]))} | {len(sentences_need_citation)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f00d90-867d-45e1-93b2-bfe52cdf0d2e",
   "metadata": {},
   "source": [
    "# Ensure there are 120 queries per method\n",
    "First, check to see how much can be kept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "125fd08b-338a-48c2-baed-7eb4f1d5a4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_trimmed_annotations_soft(df, n):\n",
    "    query_counts_by_method = df.groupby('op')['query_id'].count()\n",
    "    methods = df.groupby('op')['query_id'].count().index\n",
    "    for i in range(len(query_counts_by_method)):\n",
    "        assert query_counts_by_method.iloc[i] >= n\n",
    "        if (query_counts_by_method.iloc[i] < n):\n",
    "            print('\\tNeed more for '+methods[i]+': '+str(n-query_counts_by_method.iloc[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "2b832826-0f68-4e2d-aede-0d339d569a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "for data_str in data_str_ls:\n",
    "        df = df_dict[data_str]\n",
    "        check_trimmed_annotations_soft(df, 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "45b7bada-1871-42fa-88d6-3115d8f8de80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_trimmed_annotations(df, n):\n",
    "    query_counts_by_method = df.groupby('op')['query_id'].count()\n",
    "    for i in range(len(query_counts_by_method)):\n",
    "        assert query_counts_by_method[i] == n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "cd84961d-bed7-49ab-b853-17f3004d8ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_annotations(df, n):\n",
    "    trimmed_df = df.iloc[:0]\n",
    "    ops = np.unique(df['op'])\n",
    "    trimmed_op_df_ls = []\n",
    "    for op in ops:\n",
    "        op_df = df[df['op']==op]\n",
    "        op_df = op_df.sort_values(by='query_id')\n",
    "        op_df = op_df.iloc[:n]\n",
    "        trimmed_op_df_ls.append(op_df)\n",
    "    trimmed_df = pd.concat(trimmed_op_df_ls, ignore_index=True)\n",
    "\n",
    "    trimmed_df = trimmed_df.reset_index()\n",
    "    return trimmed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "09695113-af91-4c03-b0c1-508140ccb6b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of points removed: 190\n"
     ]
    }
   ],
   "source": [
    "n = 150\n",
    "num_trimmmed = 0\n",
    "for data_str in data_str_ls:\n",
    "    df = df_dict[data_str]\n",
    "    num_trimmmed += len(df)\n",
    "    trimmed_df = trim_annotations(df, n)\n",
    "    num_trimmmed -= len(trimmed_df)\n",
    "    df_dict[data_str] = df\n",
    "print('Number of points removed:', num_trimmmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e29268e-28f2-452a-a472-46736176601e",
   "metadata": {},
   "source": [
    "# Create the results files accounting for \"needs citation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "a6026b0d-71e0-4e6a-94a1-74f32be6fed5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showing nq Baseline=False\n",
      "nq\n",
      "[]\n",
      "Showing multihop Baseline=False\n",
      "multihop\n",
      "[]\n",
      "Showing mash Baseline=False\n",
      "mash\n",
      "[]\n",
      "Showing eli3g Baseline=False\n",
      "eli3g\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Display the cases that require review\n",
    "for data_str in data_str_ls:\n",
    "    print('Showing '+data_str+' Baseline='+str(baselines))\n",
    "    df = df_dict[data_str]\n",
    "    \n",
    "    print(data_str)\n",
    "    \n",
    "    # Identify mismatches in my sentence count and the vertex sentence count\n",
    "    identify_mismatches(df)\n",
    "    # Determine the \"needs citation\" labels by hand for this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "7fbbeb63-fb1d-49ab-8b8b-ae45ae8f3c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For cases where one or more sentences don't require citation, add their \"Sentences Need Citation\" label to a dict below\n",
    "baseline_corrections = {'mash': {}, \n",
    "               'eli3g': {70: [True]*2}, # ('Quoted', '504')\n",
    "               'nq': {},\n",
    "               'multihop': {}\n",
    "              }\n",
    "\n",
    "op_corrections = {'mash': {\n",
    "                    # 88: [False, True, True, True, True, True, True, True, True]\n",
    "                    \n",
    "    } , \n",
    "               'eli3g': {\n",
    "                #    70: [True]*2, # ('Quoted', '504')\n",
    "                   },\n",
    "               'nq': {\n",
    "                #    43: [False, False, False, True, False, True]\n",
    "                   },\n",
    "               'multihop': {}\n",
    "              }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "53530962-db2e-463a-8938-25e751ad3420",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_mismatches(df, corrections_dict):\n",
    "    for i in range(len(df)):\n",
    "        if (df['op'].iloc[i] == 'Snippet'):\n",
    "            continue\n",
    "        gpt4_sentence_count = len(eval(df['Sent'].iloc[i]))\n",
    "        vertex_sentence_count = len(eval(df['Sentences Need Citation'].iloc[i]))\n",
    "        if (gpt4_sentence_count != vertex_sentence_count):\n",
    "            if ((all(eval(df['Sentences Need Citation'].iloc[i]))) & (vertex_sentence_count > gpt4_sentence_count)):\n",
    "                 df.loc[i, 'Sentences Need Citation'] = str([True]*gpt4_sentence_count) \n",
    "            else:\n",
    "                if (i not in corrections_dict):\n",
    "                    print(i)\n",
    "                    continue\n",
    "                df.loc[i, 'Sentences Need Citation'] = str(corrections_dict[i])\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        if (df['op'].iloc[i] == 'Snippet'):\n",
    "            continue\n",
    "        gpt4_sentence_count = len(eval(df['Sent'].iloc[i]))\n",
    "        vertex_sentence_count = len(eval(df['Sentences Need Citation'].iloc[i]))\n",
    "        assert gpt4_sentence_count == vertex_sentence_count\n",
    "\n",
    "    df = df.reset_index()\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "a6c64696-278e-4e48-bc40-2bd553e3154e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed mismatches between the Vertex API and the annotated sentence count\n"
     ]
    }
   ],
   "source": [
    "# Fix the mismatches between vertex and the current sentence count\n",
    "df_dict = df_dict if baselines else df_dict\n",
    "for data_str in data_str_ls:\n",
    "    df = df_dict[data_str]\n",
    "    \n",
    "    # assign the \"needs citation\" labels for the mismatch case from above\n",
    "    if (baselines):\n",
    "        corrections_dict = baseline_corrections\n",
    "    else:\n",
    "        corrections_dict = op_corrections\n",
    "        \n",
    "    df = fix_mismatches(df, corrections_dict[data_str])  \n",
    "    df_dict[data_str] = df\n",
    "print('Fixed mismatches between the Vertex API and the annotated sentence count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "b8bd4d8a-abf2-4d4d-9de0-fbccd16e689f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_only_needs_citation(df):\n",
    "    # Remove the precision, coverage, and T2V data for sentences that do not require citation\n",
    "    # Clean up the precision and coverage annotations, given the \"needs citation labels\"\n",
    "    for i in range(len(df)):\n",
    "        if (df['op'].iloc[i] == 'Snippet'):\n",
    "            continue\n",
    "            \n",
    "        sentences_need_citation = eval(df['Sentences Need Citation'].iloc[i])\n",
    "\n",
    "        if len(sentences_need_citation) != len(eval(df['is_covered'].iloc[i])):\n",
    "            print(sentences_need_citation)\n",
    "            print(eval(df['is_covered'].iloc[i]))\n",
    "            print('OP', df['op'].iloc[i])\n",
    "            print('ID', df['query_id'].iloc[i])\n",
    "            print('id', i)\n",
    "            print()\n",
    "        \n",
    "        # first the coverage\n",
    "        is_covered = eval(df['is_covered'].iloc[i])\n",
    "        new_is_covered = []\n",
    "        for j in range(len(is_covered)):\n",
    "            sentence_idx = int(is_covered[j]['sentence_id'])\n",
    "            if (sentences_need_citation[sentence_idx]):\n",
    "                new_is_covered.append(is_covered[j])\n",
    "\n",
    "        df.loc[i, 'is_covered'] = str(new_is_covered)\n",
    "        if not (len(eval(df['is_covered'].iloc[i])) == np.sum(sentences_need_citation)):\n",
    "            print(i)\n",
    "    \n",
    "        # now the precision\n",
    "        is_precise = eval(df['precise_citations'].iloc[i])\n",
    "        new_is_precise = []\n",
    "        for j in range(len(is_precise)):\n",
    "            item = is_precise[j]\n",
    "            sentence_idx = int(item['sentence_id'])\n",
    "            if (sentences_need_citation[sentence_idx]):\n",
    "                new_is_precise.append(item)\n",
    "        df.loc[i, 'precise_citations'] = str(new_is_precise)\n",
    "        # assert len(eval(df['precise_citations'].iloc[i])) == np.sum(sentences_need_citation)\n",
    "        if not (len(eval(df['precise_citations'].iloc[i])) == np.sum(sentences_need_citation)):\n",
    "            print(i)\n",
    "        \n",
    "\n",
    "        # now T2V\n",
    "        t2vs = eval(df['t2v_coverage'].iloc[i])\n",
    "            \n",
    "        # keep the T2V values that correspond to coverage values that a) exist and b) need citation\n",
    "        actual_coverage_items = []\n",
    "        for item in is_covered:\n",
    "            if (item['coverage'] != -1):\n",
    "                actual_coverage_items.append(item)\n",
    "\n",
    "        \n",
    "        new_t2vs = []\n",
    "        for j in range(len(actual_coverage_items)):\n",
    "            sentence_idx = int(actual_coverage_items[j]['sentence_id'])\n",
    "            if (sentences_need_citation[sentence_idx]):\n",
    "                new_t2vs.append(t2vs[j])\n",
    "        df.loc[i, 't2v_coverage'] = str(new_t2vs)\n",
    "\n",
    "        # Now, handle the citations dict\n",
    "        actual_citations_dict = {}\n",
    "        citations_dict = eval(df['Citation Dict'].iloc[i])\n",
    "        for k in citations_dict.keys():\n",
    "            if (sentences_need_citation[int(k)]):\n",
    "                actual_citations_dict[k] = citations_dict[k]\n",
    "        df.loc[i, 'Citation Dict'] = str(actual_citations_dict)\n",
    "\n",
    "    df = df.reset_index()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "0f93cfdb-2d52-4114-a5df-a89490e61758",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_needs_citation(df):\n",
    "    for i in range(len(df)):\n",
    "        if (df['op'].iloc[i] == 'Snippet'):\n",
    "            continue\n",
    "        needs_citation_ls = eval(df['Sentences Need Citation'].iloc[i])\n",
    "        is_covered_ls = eval(df['is_covered'].iloc[i])\n",
    "        is_precise_ls = eval(df['precise_citations'].iloc[i])\n",
    "        assert np.sum(needs_citation_ls) == len(is_covered_ls)\n",
    "        assert  np.sum(needs_citation_ls) == len(is_precise_ls)\n",
    "        t2vs = eval(df['t2v_coverage'].iloc[i])\n",
    "        assert len(is_covered_ls) >= len(t2vs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "63625e63-899b-493e-97bc-9731db2a7b98",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nq\n",
      "multihop\n",
      "mash\n",
      "eli3g\n",
      "Discard coverage, precision, and T2V annotations for sentences that do not require citation\n"
     ]
    }
   ],
   "source": [
    "for data_str in data_str_ls:\n",
    "\n",
    "    df = df_dict[data_str]\n",
    "    # clean up the coverage, precision, and T2V annotations, given the \"needs citation labels\"\n",
    "    print(data_str)\n",
    "    df = make_only_needs_citation(df)\n",
    "\n",
    "    # check that only the relevant sentences are kept\n",
    "    check_needs_citation(df)\n",
    "    \n",
    "    df_dict[data_str] = df\n",
    "print('Discard coverage, precision, and T2V annotations for sentences that do not require citation')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d047353-2041-4b95-a08e-3f69ae770af0",
   "metadata": {},
   "source": [
    "# Save op files in the right folder with consistent naming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "771d8795-e3f4-4507-82a7-1811b9d19353",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name_ls = ['nq', 'mh', 'mash', 'eta3g']\n",
    "columns_to_remove = ['n-gram precision', 'Citation Count', 'n sentences', 'Fluency Rating', 'Perceived Utility Rating']\n",
    "\n",
    "for data_str, data_name in zip(data_str_ls, data_name_ls):\n",
    "    op_df = df_dict[data_str]\n",
    "    op_df = op_df.drop(columns=columns_to_remove)\n",
    "    op_df = op_df.loc[:, ~op_df.columns.str.startswith(\"Unnamed\")]\n",
    "    check_needs_citation(op_df) # check again that only sentences requiring citation are kept\n",
    "    if (baselines):\n",
    "        save_path = '../mturk_results/processed_results/'+data_name+'_auto_eval_byQueryOP_baselines_needs_citation_'+model_type+'.csv' # this is a file used in plotting_by_metric\n",
    "    else:\n",
    "        save_path = '../mturk_results/processed_results/'+data_name+'_auto_eval_byQueryOP_ops_needs_citation_'+model_type+'.csv' # this is a file used in plotting_by_metric\n",
    "    op_df.to_csv(save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "attrib",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
